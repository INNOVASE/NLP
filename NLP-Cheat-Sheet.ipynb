{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Cheat Sheet - Introduction - Overview - Python - Starter Kit\n",
    "\n",
    "Introduction to Natural Language Processing (NLP) tools, frameworks, concepts, resources for Python\n",
    "\n",
    "Demo: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/janlukasschroeder/nlp-cheat-sheet-python/master?filepath=NLP%2BCheat%2BSheet.ipynb)\n",
    "\n",
    "\n",
    "# NLP Python Libraries\n",
    "- [spacy](https://spacy.io/)\n",
    "- [NLTK](https://www.nltk.org/) - similar to spacy, supports more models, simpl GUI model download `nltk.download()`\n",
    "- [gensim](https://radimrehurek.com/gensim) - topic modelling, accessing corpus, similarity calculations between query and indexed docs, SparseMatrixSimilarity, Latent Semantic Analysis\n",
    "- [lexnlp](https://github.com/LexPredict/lexpredict-lexnlp) - information retrieval and extraction for real, unstructured legal text\n",
    "- [Holmes](https://github.com/msg-systems/holmes-extractor#the-basic-idea) - information extraction, document classification, search in documents\n",
    "- [Pytorch-Transformers - includes BERT, GPT2, XLNet](https://huggingface.co/pytorch-transformers/index.html)\n",
    "\n",
    "Uncased model is better unless you know that case information is important for your task (e.g., Named Entity Recognition or Part-of-Speech tagging)\n",
    "\n",
    "###  General\n",
    "- PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing\n",
    "- Tensorflow\n",
    "- Keras\n",
    "\n",
    "\n",
    "# NLP Algortihms\n",
    "- [GPT-2](https://openai.com/blog/better-language-models/) - generate fake news, text summaries\n",
    "- [BERT](https://github.com/google-research/bert)\n",
    "- [FinBERT](https://github.com/ProsusAI/finBERT) - analyze sentiment of financial text\n",
    "- [XLnet](https://github.com/zihangdai/xlnet)\n",
    "- [ERNIE](https://github.com/PaddlePaddle/ERNIE/)\n",
    "\n",
    "\n",
    "\n",
    "# Datasets\n",
    "\n",
    "- [Gutenberg Corpus](https://block.pglaf.org/germany.shtml) - contains 25,000 free electronic books. `from nltk.corpus import gutenberg`\n",
    "- [OntoNotes 5](https://github.com/ontonotes/conll-formatted-ontonotes-5.0) - corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).\n",
    "- [wiki_en_tfidf.mm in gensim](https://radimrehurek.com/gensim/wiki.html#latent-semantic-analysis) 3.9M documents, 100K features (distinct tokens) and 0.76G non-zero entries in the sparse TF-IDF matrix. The Wikipedia corpus contains about 2.24 billion tokens in total.\n",
    "- [GPT-2 Dataset](https://github.com/openai/gpt-2-output-dataset)\n",
    "- [Brown corpus](http://icame.uib.no/brown/bcm-los.html) - contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on.\n",
    "- [Reuters Corpus - 10,788 news documents totaling 1.3 million words](https://www.nltk.org/book/ch02.html)\n",
    "- [Newsfilter.io stock market news corpus](https://developers.newsfilter.io/) - contains over 4 million press releases, earnings reports, FDA drug approvals, analyst ratings, merger agreements and many more covering all US companies listed on NASDAQ, NYSE, AMEX\n",
    "- [Kaggle - All the news, 143K articles](https://www.kaggle.com/snapcrack/all-the-news)\n",
    "- [Kaggle - Daily news for stock market prediction](https://www.kaggle.com/aaron7sun/stocknews)\n",
    "- [CNN News](https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ)\n",
    "- [AG News - PyTorch integrated](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)\n",
    "\n",
    "### Installation:\n",
    "spacy (good for beginners; use NLTK for bigger projects)\n",
    "```shell\n",
    "pip install spacy\n",
    "python -m spacy download en \n",
    "# python -m spacy download en_core_web_lg\n",
    "```\n",
    "LexNLP (good for dealing with legal and financial documents; [installation guide here](https://github.com/LexPredict/lexpredict-contraxsuite/blob/master/documentation/Installation%20and%20Configuration/Installation%20and%20Configuration%20Guide.pdf))\n",
    "```shell\n",
    "pip install https://github.com/LexPredict/lexpredict-lexnlp/archive/master.zip\n",
    "python # to open REPL console\n",
    ">>> import nltk\n",
    ">>> nltk.download() # download all packages\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings (=word vectors)\n",
    "\n",
    "Visualizing word vectors using PCA. Paper: https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "![word-embs](https://www.tensorflow.org/images/linear-relationships.png)\n",
    "\n",
    "- Word embeddings are vector representation of words. \n",
    "- Example sentence: word embeddings are words converted into numbers.\n",
    "- A word in this sentence may be “Embeddings” or “numbers ” etc.\n",
    "- A dictionary may be the list of all unique words in the sentence, eg [‘Word’,’Embeddings’,’are’,’Converted’,’into’,’numbers’]\n",
    "- A vector representation of a word may be a one-hot encoded vector where 1 stands for the position where the word exists and 0 everywhere else. \n",
    "\n",
    "### Example\n",
    "\n",
    "- numbers = [0,0,0,0,0,1] \n",
    "- converted = [0,0,0,1,0,0]\n",
    "\n",
    "** Either use pre-trained word vectors or train our own** \n",
    "\n",
    "#### Pre-trained word embeddings:\n",
    "- Word2Vec (Google, 2013), uses Skip Gram and CBOW\n",
    "- [Vectors trained on Google News (1.5GB)](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit) - vocabulary of 3 million words trained on around 100 billion words from the google news dataset\n",
    "- GloVe (Stanford)\n",
    "- [Stanford Named Entity Recognizer (NER)](https://nlp.stanford.edu/software/CRF-NER.shtml)\n",
    "- [LexPredict: pre-trained word embedding models for legal or regulatory text](https://github.com/LexPredict/lexpredict-lexnlp)\n",
    "- [LexNLP legal models](https://github.com/LexPredict/lexpredict-legal-dictionary) - US GAAP, finaical common terms, US federal regulators, common law\n",
    "\n",
    "#### Create word vectors yourself\n",
    "\n",
    "```python\n",
    "import gensim\n",
    "word2vev_model = gensim.models.word2vec.Word2Vec(sentence_list)\n",
    "```\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create word vectors?\n",
    "- Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word. \n",
    "- Predictive models directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors (considered parameters of the model).\n",
    "  - Example: Word2vec (Google)\n",
    "\n",
    "### 1. Count based word embeddings\n",
    "\n",
    "#### Count Vector (= Document Term Matrix)\n",
    "\n",
    "![img](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04164920/count-vector.png)\n",
    "\n",
    "#### TF-IDF\n",
    "\n",
    "Term Frequency - Inverse Document Frequency\n",
    "\n",
    "- Term frequency equals the number of times a word appears in a document divided by the total number of words in the document. \n",
    "- Inverse document frequency calculates the weight of rare words in all documents in the corpus, with rare words having a high IDF score, and words that are present in all documents in a corpus having IDF close to zero.\n",
    "\n",
    "(sklearn) in Python has a function TfidfVectorizer() that will compute the TF-IDF values for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Write a function for cleaning strings and returning an array of ngrams\n",
    "def ngrams_analyzer(string):\n",
    "    string = re.sub(r'[,-./]', r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(5)])  # N-Gram length is 5\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "# Construct your vectorizer for building the TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(analyzer=ngrams_analyzer)\n",
    "\n",
    "# Credits: https://towardsdatascience.com/group-thousands-of-similar-spreadsheet-text-cells-in-seconds-2493b3ce6d8d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Co-Occurrence Vector\n",
    "\n",
    "\n",
    "### 2. Prediction based word embeddings\n",
    "\n",
    "- Uses Neural Networks\n",
    "- CBOW predicts target words (e.g. 'mat') from source context words ('the cat sits on the')\n",
    "- Skip-gram does the inverse and predicts source context-words from the target words\n",
    "\n",
    "#### CBOW (Continuous Bag of words)\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/softmax-nplm.png\" width=\"400\">\n",
    "\n",
    "#### Skip Gram\n",
    "\n",
    "Skip – gram follows the same topology as of CBOW. It just flips CBOW’s architecture on its head. The aim of skip-gram is to predict the context given a word\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/nce-nplm.png\" width=\"400\">\n",
    "\n",
    "#### Outcome\n",
    "\n",
    "![out](https://github.com/sanketg10/deep-learning-repo/raw/6b207e326cc937930a0512a8c599e86e48c297b2/embeddings/assets/skip_gram_net_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# John likes to watch movies. Mary likes movies too.\n",
    "BoW1 = {\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "nlp = spacy.load(\"en\")\n",
    "# Import large dataset. Needs to be downloaded first.\n",
    "# nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "\n",
    "Stop words are the very common words like ‘if’, ‘but’, ‘we’, ‘he’, ‘she’, and ‘they’.\n",
    "We can usually remove these words without changing the semantics of a text and doing so often (but not always) improves the performance of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy: Number of stop words: 326\n"
     ]
    }
   ],
   "source": [
    "# spacy: Removing stop words\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "print('spacy: Number of stop words: %d' % len(spacy_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntlk: Number of stop words: 179\n"
     ]
    }
   ],
   "source": [
    "# nltk: Removing stop words \n",
    "from nltk.corpus import stopwords\n",
    "english_stop_words = stopwords.words('english')\n",
    "\n",
    "print('ntlk: Number of stop words: %d' % len(english_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Larry Page founded Google in early 1990.\n",
      "\n",
      "['Larry', 'Page', 'founded', 'Google', 'early', '1990', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Larry Page founded Google in early 1990.'\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc if not token.is_stop]\n",
    "print('Original text: %s' % (text))\n",
    "print()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spans\n",
    "Part of a given text. So doc[2:4] is a span starting at token 2, up to – but not including! – token 4.\n",
    "\n",
    "Docs: https://spacy.io/api/span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'founded Google'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Larry Page founded Google in early 1990.\")\n",
    "span = doc[2:4]\n",
    "span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Larry, Page, founded, Google, in, early, 1990, .]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(spans) for spans in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token and Tokenization\n",
    "\n",
    "Segmenting text into words, punctuation etc.\n",
    "- Sentence tokenization\n",
    "- Word tokenization\n",
    "\n",
    "Docs: https://spacy.io/api/token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Larry', 'Page', 'founded', 'Google', 'in', 'early', '1990', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Larry Page founded Google in early 1990.\")\n",
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI GPT-2 using PyTorch Transformers\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "# https://huggingface.co/pytorch-transformers/serialization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunks and Chunking\n",
    "\n",
    "Segments and labels multi-token sequences.\n",
    "\n",
    "- Each of these larger boxes is called a chunk. \n",
    "- Like tokenization, which omits whitespace, chunking usually selects a subset of the tokens. \n",
    "- The pieces produced by a chunker do not overlap in the source text.\n",
    "\n",
    "<img src=\"https://www.nltk.org/images/chunk-segmentation.png\" width=\"400\">\n",
    "<center>Segmentation and Labeling at both the Token and Chunk Levels</center>\n",
    "\n",
    "<img src=\"https://www.nltk.org/images/chunk-tagrep.png\" width=\"400\">\n",
    "<center>Tag Representation of Chunk Structures</center>\n",
    "\n",
    "<img src=\"https://www.nltk.org/images/chunk-treerep.png\" width=\"400\">\n",
    "<center>Tree Representation of Chunk Structures</center>\n",
    "\n",
    "Credits: https://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinks and Chinking\n",
    "\n",
    "Chink is a sequence of tokens that is not included in a chunk.\n",
    "\n",
    "Credits: https://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech (POS) Tagging\n",
    "\n",
    "Assigning word types to tokens like verb or noun.\n",
    "\n",
    "POS tagging should be done straight after tokenization and before any words are removed so that sentence structure is preserved and it is more obvious what part of speech the word belongs to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Asian', 'ADJ', 'adjective'),\n",
       " ('shares', 'NOUN', 'noun'),\n",
       " ('skidded', 'VERB', 'verb'),\n",
       " ('on', 'ADP', 'adposition'),\n",
       " ('Tuesday', 'PROPN', 'proper noun'),\n",
       " ('after', 'ADP', 'adposition'),\n",
       " ('a', 'DET', 'determiner'),\n",
       " ('rout', 'NOUN', 'noun'),\n",
       " ('in', 'ADP', 'adposition'),\n",
       " ('tech', 'NOUN', 'noun'),\n",
       " ('stocks', 'NOUN', 'noun'),\n",
       " ('put', 'VERB', 'verb'),\n",
       " ('Wall', 'PROPN', 'proper noun'),\n",
       " ('Street', 'PROPN', 'proper noun'),\n",
       " ('to', 'ADP', 'adposition'),\n",
       " ('the', 'DET', 'determiner'),\n",
       " ('sword', 'NOUN', 'noun')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Asian shares skidded on Tuesday after a rout in tech stocks put Wall Street to the sword\"\n",
    "doc = nlp(text)\n",
    "[(x.orth_, x.pos_, spacy.explain(x.pos_)) for x in [token for token in doc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Asian', 'JJ', 'adjective'),\n",
       " ('shares', 'NNS', 'noun, plural'),\n",
       " ('skidded', 'VBD', 'verb, past tense'),\n",
       " ('on', 'IN', 'conjunction, subordinating or preposition'),\n",
       " ('Tuesday', 'NNP', 'noun, proper singular'),\n",
       " ('after', 'IN', 'conjunction, subordinating or preposition'),\n",
       " ('a', 'DT', 'determiner'),\n",
       " ('rout', 'NN', 'noun, singular or mass'),\n",
       " ('in', 'IN', 'conjunction, subordinating or preposition'),\n",
       " ('tech', 'NN', 'noun, singular or mass'),\n",
       " ('stocks', 'NNS', 'noun, plural'),\n",
       " ('put', 'VBD', 'verb, past tense'),\n",
       " ('Wall', 'NNP', 'noun, proper singular'),\n",
       " ('Street', 'NNP', 'noun, proper singular'),\n",
       " ('to', 'IN', 'conjunction, subordinating or preposition'),\n",
       " ('the', 'DT', 'determiner'),\n",
       " ('sword', 'NN', 'noun, singular or mass')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x.orth_, x.tag_, spacy.explain(x.tag_)) for x in [token for token in doc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Asian', 'JJ'),\n",
       " ('shares', 'NNS'),\n",
       " ('skidded', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('Tuesday', 'NNP'),\n",
       " ('after', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('rout', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('tech', 'JJ'),\n",
       " ('stocks', 'NNS'),\n",
       " ('put', 'VBD'),\n",
       " ('Wall', 'NNP'),\n",
       " ('Street', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('sword', 'NN')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using nltk\n",
    "import nltk\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BILUO tagging\n",
    "\n",
    "- BEGIN - The first token of a multi-token entity.\n",
    "- IN - An inner token of a multi-token entity.\n",
    "- LAST - The final token of a multi-token entity.\n",
    "- UNIT - A single-token entity.\n",
    "- OUT - A non-entity token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Asian, 'B', 'NORP'),\n",
       " (shares, 'O', ''),\n",
       " (skidded, 'O', ''),\n",
       " (on, 'O', ''),\n",
       " (Tuesday, 'B', 'DATE'),\n",
       " (after, 'O', ''),\n",
       " (a, 'O', ''),\n",
       " (rout, 'O', ''),\n",
       " (in, 'O', ''),\n",
       " (tech, 'O', ''),\n",
       " (stocks, 'O', ''),\n",
       " (put, 'O', ''),\n",
       " (Wall, 'O', ''),\n",
       " (Street, 'O', ''),\n",
       " (to, 'O', ''),\n",
       " (the, 'O', ''),\n",
       " (sword, 'O', '')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.ent_iob_, token.ent_type_) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Stemming is the process of reducing words to their root form.\n",
    "\n",
    "Examples:\n",
    "- cats, catlike, catty → cat\n",
    "- fishing, fished, fisher → fish\n",
    "\n",
    "There are two types of stemmers in NLTK: [Porter Stemmer](https://tartarus.org/martin/PorterStemmer/) and [Snowball stemmers](https://tartarus.org/martin/PorterStemmer/)\n",
    "\n",
    "[Credits](https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute --> comput\n",
      "computer --> comput\n",
      "computed --> comput\n",
      "computing --> comput\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "tokens = ['compute', 'computer', 'computed', 'computing']\n",
    "for token in tokens:\n",
    "    print(token + ' --> ' + stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Assigning the base form of word, for example: \n",
    "- \"was\" → \"be\"\n",
    "- \"rats\" → \"rat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Was', 'be'),\n",
       " ('Google', 'Google'),\n",
       " ('founded', 'found'),\n",
       " ('in', 'in'),\n",
       " ('early', 'early'),\n",
       " ('1990', '1990'),\n",
       " ('?', '?')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Was Google founded in early 1990?\")\n",
    "[(x.orth_, x.lemma_) for x in [token for token in doc]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Detection\n",
    "\n",
    "Finding and segmenting individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Larry Page founded Google in early 1990.', 'Sergey Brin joined.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Larry Page founded Google in early 1990. Sergey Brin joined.\")\n",
    "[sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing\t\n",
    "\n",
    "Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'nsubj', 'nominal subject'),\n",
       " ('are', 'aux', 'auxiliary'),\n",
       " ('reading', 'ROOT', None),\n",
       " ('a', 'det', 'determiner'),\n",
       " ('text', 'dobj', 'direct object'),\n",
       " ('.', 'punct', 'punctuation')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"We are reading a text.\")\n",
    "# Dependency labels\n",
    "[(x.orth_, x.dep_, spacy.explain(x.dep_)) for x in [token for token in doc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reading', 'reading', 'reading', 'text', 'reading', 'reading']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Syntactic head token (governor)\n",
    "[token.head.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base noun phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'a red car']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I have a red car\")\n",
    "[chunk.text for chunk in doc.noun_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "What is NER? Labeling \"real-world\" objects, like persons, companies or locations.\n",
    "\n",
    "2 popular approaches:\n",
    "- Rule-based\n",
    "- ML-based:\n",
    "    - Multi-class classification\n",
    "    - Conditional Random Field (probabilistic graphical model)\n",
    "\n",
    "Datasets:\n",
    "- [Kaggle, IOB, POS tags](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/home)\n",
    "\n",
    "Credits: https://medium.com/@yingbiao/ner-with-bert-in-action-936ff275bc73\n",
    "\n",
    "\n",
    "Entities supported by spacy:\n",
    "- PERSON\tPeople, including fictional.\n",
    "- NORP\tNationalities or religious or political groups.\n",
    "- FAC\tBuildings, airports, highways, bridges, etc.\n",
    "- ORG\tCompanies, agencies, institutions, etc.\n",
    "- GPE\tCountries, cities, states.\n",
    "- LOC\tNon-GPE locations, mountain ranges, bodies of water.\n",
    "- PRODUCT\tObjects, vehicles, foods, etc. (Not services.)\n",
    "- EVENT\tNamed hurricanes, battles, wars, sports events, etc.\n",
    "- WORK_OF_ART\tTitles of books, songs, etc.\n",
    "- LAW\tNamed documents made into laws.\n",
    "- LANGUAGE\tAny named language.\n",
    "- DATE\tAbsolute or relative dates or periods.\n",
    "- TIME\tTimes smaller than a day.\n",
    "- PERCENT\tPercentage, including ”%“.\n",
    "- MONEY\tMonetary values, including unit.\n",
    "- QUANTITY\tMeasurements, as of weight or distance.\n",
    "- ORDINAL\t“first”, “second”, etc.\n",
    "- CARDINAL\tNumerals that do not fall under another type.\n",
    "\n",
    "## Alternatives to spacy\n",
    "\n",
    "[LexNLP](https://lexpredict-lexnlp.readthedocs.io/en/latest/modules/extract/extract.html#pattern-based-extraction-methods) entities:\n",
    "- acts, e.g., “section 1 of the Advancing Hope Act, 1986”\n",
    "- amounts, e.g., “ten pounds” or “5.8 megawatts”\n",
    "- citations, e.g., “10 U.S. 100” or “1998 S. Ct. 1”\n",
    "- companies, e.g., “Lexpredict LLC”\n",
    "- conditions, e.g., “subject to …” or “unless and until …”\n",
    "- constraints, e.g., “no more than” or “\n",
    "- copyright, e.g., “(C) Copyright 2000 Acme”\n",
    "- courts, e.g., “Supreme Court of New York”\n",
    "- CUSIP, e.g., “392690QT3”\n",
    "- dates, e.g., “June 1, 2017” or “2018-01-01”\n",
    "- definitions, e.g., “Term shall mean …”\n",
    "- distances, e.g., “fifteen miles”\n",
    "- durations, e.g., “ten years” or “thirty days”\n",
    "- geographic and geopolitical entities, e.g., “New York” or “Norway”\n",
    "- money and currency usages, e.g., “$5” or “10 Euro”\n",
    "- percents and rates, e.g., “10%” or “50 bps”\n",
    "- PII, e.g., “212-212-2121” or “999-999-9999”\n",
    "- ratios, e.g.,” 3:1” or “four to three”\n",
    "- regulations, e.g., “32 CFR 170”\n",
    "- trademarks, e.g., “MyApp (TM)”\n",
    "- URLs, e.g., “http://acme.com/”\n",
    "\n",
    "Stanford NER entities:\n",
    "- Location, Person, Organization, Money, Percent, Date, Time\n",
    "\n",
    "NLTK\n",
    "- NLTK maximum entropy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Larry Page', 'PERSON'),\n",
       " ('Google', 'ORG'),\n",
       " ('US', 'GPE'),\n",
       " ('early 1990', 'DATE')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Larry Page founded Google in the US in early 1990.\")\n",
    "# Text and label of named entity span\n",
    "[(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('European', 'NORP'),\n",
       " ('Google', 'ORG'),\n",
       " ('$5.1 billion', 'MONEY'),\n",
       " ('Wednesday', 'DATE')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
    "[(X.text, X.label_) for X in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'NORP': 1, 'ORG': 1, 'MONEY': 1, 'DATE': 1})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "labels = [x.label_ for x in doc.ents]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(European, 'B', 'NORP'),\n",
       " (authorities, 'O', ''),\n",
       " (fined, 'O', ''),\n",
       " (Google, 'B', 'ORG'),\n",
       " (a, 'O', ''),\n",
       " (record, 'O', ''),\n",
       " ($, 'B', 'MONEY'),\n",
       " (5.1, 'I', 'MONEY'),\n",
       " (billion, 'I', 'MONEY'),\n",
       " (on, 'O', ''),\n",
       " (Wednesday, 'B', 'DATE'),\n",
       " (for, 'O', ''),\n",
       " (abusing, 'O', ''),\n",
       " (its, 'O', ''),\n",
       " (power, 'O', ''),\n",
       " (in, 'O', ''),\n",
       " (the, 'O', ''),\n",
       " (mobile, 'O', ''),\n",
       " (phone, 'O', ''),\n",
       " (market, 'O', ''),\n",
       " (and, 'O', ''),\n",
       " (ordered, 'O', ''),\n",
       " (the, 'O', ''),\n",
       " (company, 'O', ''),\n",
       " (to, 'O', ''),\n",
       " (alter, 'O', ''),\n",
       " (its, 'O', ''),\n",
       " (practices, 'O', '')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(X, X.ent_iob_, X.ent_type_) for X in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['European', 'Google', '$5.1 billion', 'Wednesday']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('European', 1), ('Google', 1), ('$5.1 billion', 1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Begin and In entities\n",
    "items = [x.text for x in doc.ents]\n",
    "print(items)\n",
    "Counter(items).most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lexnlp.extract.en as lexnlp\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 2.0]\n"
     ]
    }
   ],
   "source": [
    "text = \"There are ten cows in the 2 acre pasture.\"\n",
    "print(list(lexnlp.amounts.get_amounts(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'location_start': 5,\n",
       "  'location_end': 49,\n",
       "  'act_name': 'VERY Important Act',\n",
       "  'section': '12',\n",
       "  'year': '1954',\n",
       "  'ambiguous': False,\n",
       "  'value': 'section 12 of the VERY Important Act of 1954'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lexnlp.extract.en.acts\n",
    "text = \"test section 12 of the VERY Important Act of 1954.\"\n",
    "lexnlp.extract.en.acts.get_act_list(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "Two types:\n",
    "- binary classification (text only belongs to one class)\n",
    "- multi-class classification (text can belong to multiple classes)\n",
    "\n",
    "Assigning categories or labels to a whole document, or parts of a document.\n",
    "\n",
    "Approach:\n",
    "- calculate document vectors for each document\n",
    "- use kNN to calculate clusters based on document vectors\n",
    "- each cluster represents a class of documents that are similar to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Credits: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "vocab = train_dataset.get_vocab()\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])\n",
    "\n",
    "# Output: This is a Sports news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer \n",
    "- Convert a collection of text documents to a matrix of token counts\n",
    "- [skikitLearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity\n",
    "How similar are two documents, sentences, token or spans?\n",
    "\n",
    "Cosine similarity (also known as: L2-normalized dot product of vectors) is a formula used to calculate how similar two given word vectors are.\n",
    "\n",
    "How to calculate Cosine similarity?\n",
    "- spacy (see example below)\n",
    "- scikit: [sklearn.metrics.pairwise.cosine_similarity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "Figure below shows three word vectors and Cosine distance (=similarity) between \n",
    "- \"I hate cats\" and \"I love dogs\" (result: not very similar)\n",
    "- \"I love dogs\" and \"I love, love, love, .. dogs\" (result: similar)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*QNp4TNCNwo1HMqjFV4jq1g.png\" width=\"600\">\n",
    "\n",
    "[Credits](https://towardsdatascience.com/group-thousands-of-similar-spreadsheet-text-cells-in-seconds-2493b3ce6d8d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Norm\n",
    "\n",
    "Length of a word vector. Also known as **Euclidean norm**.\n",
    "\n",
    "Example:\n",
    "- length of \"I like cats\" is 4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.957709143352323"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I like dogs\")\n",
    "# Compare 2 documents\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83117634"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"cats\" vs \"dogs\"\n",
    "doc1[2].similarity(doc2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46475163"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"I\" vs \"like dogs\"\n",
    "doc1[0].similarity(doc2[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.706799587675896"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I like cats\")\n",
    "# L2 norm of \"I like cats\"\n",
    "doc.vector_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.933004"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 norm of \"cats\"\n",
    "doc[2].vector_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.26763  ,  0.029846 , -0.3437   , -0.54409  , -0.49919  ,\n",
       "        0.15928  , -0.35278  , -0.2036   ,  0.23482  ,  1.5671   ,\n",
       "       -0.36458  , -0.028713 , -0.27053  ,  0.2504   , -0.18126  ,\n",
       "        0.13453  ,  0.25795  ,  0.93213  , -0.12841  , -0.18505  ,\n",
       "       -0.57597  ,  0.18538  , -0.19147  , -0.38465  ,  0.21656  ,\n",
       "       -0.4387   , -0.27846  , -0.41339  ,  0.37859  , -0.2199   ,\n",
       "       -0.25907  , -0.019796 , -0.31885  ,  0.12921  ,  0.22168  ,\n",
       "        0.32671  ,  0.46943  , -0.81922  , -0.20031  ,  0.013561 ,\n",
       "       -0.14663  ,  0.14438  ,  0.0098044, -0.15439  ,  0.21146  ,\n",
       "       -0.28409  , -0.4036   ,  0.45355  ,  0.12173  , -0.11516  ,\n",
       "       -0.12235  , -0.096467 , -0.26991  ,  0.028776 , -0.11307  ,\n",
       "        0.37219  , -0.054718 , -0.20297  , -0.23974  ,  0.86271  ,\n",
       "        0.25602  , -0.3064   ,  0.014714 , -0.086497 , -0.079054 ,\n",
       "       -0.33109  ,  0.54892  ,  0.20076  ,  0.28064  ,  0.037788 ,\n",
       "        0.0076729, -0.0050123, -0.11619  , -0.23804  ,  0.33027  ,\n",
       "        0.26034  , -0.20615  , -0.35744  ,  0.54125  , -0.3239   ,\n",
       "        0.093441 ,  0.17113  , -0.41533  ,  0.13702  , -0.21765  ,\n",
       "       -0.65442  ,  0.75733  ,  0.359    ,  0.62492  ,  0.019685 ,\n",
       "        0.21156  ,  0.28125  ,  0.22288  ,  0.026787 , -0.1019   ,\n",
       "        0.11178  ,  0.17202  , -0.20403  , -0.01767  , -0.34351  ,\n",
       "        0.11926  ,  0.73156  ,  0.11094  ,  0.12576  ,  0.64825  ,\n",
       "       -0.80004  ,  0.62074  , -0.38557  ,  0.015614 ,  0.2664   ,\n",
       "        0.18254  ,  0.11678  ,  0.58919  , -1.0639   , -0.29969  ,\n",
       "        0.14827  , -0.42925  , -0.090766 ,  0.12313  , -0.024253 ,\n",
       "       -0.21265  , -0.10331  ,  0.91988  , -1.4097   , -0.0542   ,\n",
       "       -0.071201 ,  0.66878  , -0.24651  , -0.46788  , -0.23991  ,\n",
       "       -0.14138  , -0.038911 , -0.48678  ,  0.22975  ,  0.36074  ,\n",
       "        0.13024  , -0.40091  ,  0.19673  ,  0.016017 ,  0.30575  ,\n",
       "       -2.1901   , -0.55468  ,  0.26955  ,  0.63815  ,  0.42724  ,\n",
       "       -0.070186 , -0.11196  ,  0.14079  , -0.022228 ,  0.070456 ,\n",
       "        0.17229  ,  0.099383 , -0.12258  , -0.23416  , -0.26525  ,\n",
       "       -0.088991 , -0.061554 ,  0.26582  , -0.53112  , -0.4106   ,\n",
       "        0.45211  , -0.39669  , -0.43746  , -0.6632   , -0.048135 ,\n",
       "        0.23171  , -0.37665  , -0.38261  , -0.29286  , -0.036613 ,\n",
       "        0.25354  ,  0.49775  ,  0.3359   , -0.11285  , -0.17228  ,\n",
       "        0.85991  , -0.34081  ,  0.27959  ,  0.03698  ,  0.61782  ,\n",
       "        0.23739  , -0.32049  , -0.073717 ,  0.015991 , -0.37395  ,\n",
       "       -0.4152   ,  0.049221 , -0.3137   ,  0.091128 , -0.38258  ,\n",
       "       -0.036783 ,  0.10902  , -0.38332  , -0.74754  ,  0.016473 ,\n",
       "        0.55256  , -0.29053  , -0.50617  ,  0.83599  , -0.31783  ,\n",
       "       -0.77465  , -0.0049272, -0.17103  , -0.38067  ,  0.44987  ,\n",
       "       -0.12497  ,  0.60263  , -0.12026  ,  0.37368  , -0.079952 ,\n",
       "       -0.15785  ,  0.37684  , -0.18679  ,  0.18855  , -0.4759   ,\n",
       "       -0.11708  ,  0.36999  ,  0.54134  ,  0.42752  ,  0.038618 ,\n",
       "        0.043483 ,  0.31435  , -0.24491  , -0.67818  , -0.33833  ,\n",
       "        0.039218 , -0.11964  ,  0.8474   ,  0.09451  ,  0.070523 ,\n",
       "       -0.2806   ,  0.296    , -0.17554  , -0.41087  ,  0.70748  ,\n",
       "        0.17686  ,  0.043479 , -0.31902  ,  0.64584  , -0.45268  ,\n",
       "       -0.7967   ,  0.099817 , -0.1734   ,  0.11404  , -0.36809  ,\n",
       "        0.12035  , -0.048582 ,  0.55945  , -0.51508  ,  0.072704 ,\n",
       "        0.18106  ,  0.07802  , -0.31526  ,  0.38189  ,  0.092801 ,\n",
       "       -0.044227 , -0.66154  , -0.020428 ,  0.059836 , -0.23628  ,\n",
       "       -0.017592 , -0.56481  , -0.52934  , -0.16392  ,  0.077331 ,\n",
       "        0.24583  , -0.32195  , -0.36811  , -0.037208 ,  0.26702  ,\n",
       "       -0.57907  ,  0.46457  , -0.54636  ,  0.11855  ,  0.092475 ,\n",
       "       -0.10469  ,  0.03319  ,  0.62616  , -0.33684  ,  0.045742 ,\n",
       "        0.25089  ,  0.28973  ,  0.060633 , -0.4096   ,  0.39198  ,\n",
       "        0.58276  ,  0.496    , -0.75881  ,  0.13655  ,  0.21704  ,\n",
       "       -0.37978  , -0.54051  , -0.22813  ,  0.28393  , -0.58739  ,\n",
       "        1.0472   , -0.13318  , -0.07325  ,  0.12991  , -0.44999  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector representation of \"cats\"\n",
    "doc[2].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# can also be done using sklearn's linear kernel (equivilant to cosine similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams: Unigram, bigrams, trigrams\n",
    "\n",
    "- Unigram = one word, eg the, and, of, hotel\n",
    "- Bigrams = two consecutive words, eg the hotel, in seattle, the city\n",
    "- Trigrams = three consecutive words, eg easy access to, high speed internet, the heart of\n",
    "\n",
    "Credits: https://towardsdatascience.com/building-a-content-based-recommender-system-for-hotels-in-seattle-d724f0a32070"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-566c98ebc72f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mcommon_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_n_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# or use df['desc']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'desc'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'desc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'barh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Top 20 words in hotel description after removing stop words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "\n",
    "common_words = get_top_n_words(text, 20) # or use df['desc'] \n",
    "\n",
    "df2 = pd.DataFrame(common_words, columns = ['desc' , 'count'])\n",
    "df2.groupby('desc').sum()['count'].sort_values().plot(kind='barh', title='Top 20 words in hotel description after removing stop words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_bigram(df['desc'], 20)\n",
    "df4 = pd.DataFrame(common_words, columns = ['desc' , 'count'])\n",
    "df4.groupby('desc').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in hotel description After removing stop words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_trigram(df['desc'], 20)\n",
    "df6 = pd.DataFrame(common_words, columns = ['desc' , 'count'])\n",
    "df6.groupby('desc').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 trigrams in hotel description after removing stop words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"0bf72b5ccfc143b4871b353f00ec3470-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">sentence</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0bf72b5ccfc143b4871b353f00ec3470-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0bf72b5ccfc143b4871b353f00ec3470-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0bf72b5ccfc143b4871b353f00ec3470-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0bf72b5ccfc143b4871b353f00ec3470-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0bf72b5ccfc143b4871b353f00ec3470-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0bf72b5ccfc143b4871b353f00ec3470-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"This is a sentence\")\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Larry Page\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " founded \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    US\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    early 1990\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"Larry Page founded Google in the US in early 1990.\")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by: https://www.datacamp.com/community/blog/spacy-cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kernels \n",
    "\n",
    "Used by \n",
    "- Support Vector Machines (SVMs)\n",
    "- Principal Component Analysis (PCA)\n",
    "\n",
    "Useful for\n",
    "- classification tasks\n",
    "\n",
    "Also known as\n",
    "- kernel function\n",
    "- similarity function\n",
    "\n",
    "Opposite of kernels: vectors\n",
    "\n",
    "Source:\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Kernel_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Kernel\n",
    "\n",
    "Linear Kernel is used when the data is Linearly separable, that is, it can be separated using a single Line.\n",
    "\n",
    "Compute the linear kernel between X and Y: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.linear_kernel.html#sklearn.metrics.pairwise.linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Kernel\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Kernel_trick_idea.svg/1280px-Kernel_trick_idea.svg.png\" width=\"600\">\n",
    "\n",
    "[Credits](https://en.wikipedia.org/wiki/Kernel_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman's Rank Correlation Coefficient\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Spearman_fig1.svg/1280px-Spearman_fig1.svg.png\" width=\"300\">\n",
    "\n",
    "Credits: https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN\n",
    "\n",
    "k-nearest neighbors algoritm\n",
    "\n",
    "Useful for\n",
    "- classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization\n",
    "\n",
    "- [How to Make a Text Summarizer](https://github.com/llSourcell/How_to_make_a_text_summarizer)\n",
    "- [How to Prepare News Articles for Text Summarization](https://machinelearningmastery.com/prepare-news-articles-text-summarization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Is text fact or opinion? Only perform sentiment analysis on opinion, not facts.\n",
    "\n",
    "Sentiments:\n",
    "- positive\n",
    "- neutral\n",
    "- negative\n",
    "\n",
    "2 ways:\n",
    "- rule-based uses lexicon with polarity score per word. Count positive and negative words. Doesn't provide training data.\n",
    "- automatic using machine learning (=classification problem). Needs training data.\n",
    "\n",
    "Sentiment analysis can be performed with ntlk's `SentimentIntensityAnalyzer`\n",
    "\n",
    "See: https://www.nltk.org/api/nltk.sentiment.html#module-nltk.sentiment.vader\n",
    "\n",
    "Learning resources: \n",
    "- https://www.youtube.com/watch?v=3Pzni2yfGUQ\n",
    "- https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/PycharmProjects/playground/nlp-cheat-sheet/venv/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "A classification model that uses a sigmoid function to convert a linear model's raw prediction () into a value between 0 and 1. You can interpret the value between 0 and 1 in either of the following two ways:\n",
    "\n",
    "- As a probability that the example belongs to the positive class in a binary classification problem.\n",
    "- As a value to be compared against a classification threshold. If the value is equal to or above the classification threshold, the system classifies the example as the positive class. Conversely, if the value is below the given threshold, the system classifies the example as the negative class.\n",
    "\n",
    "https://developers.google.com/machine-learning/glossary/#logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Recurrent neural networks\n",
    "- Size changes depending on input/output (in contrast to neural network like CNN)\n",
    "\n",
    "## LSTM\n",
    "\n",
    "Long Short-Term Mermoy\n",
    "\n",
    "ToDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-61823dfb33e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRepeatVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMerge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeDistributedDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector, Merge, TimeDistributedDense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Levenshtein'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-4945bd004c73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mLevenshtein\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Levenshtein'"
     ]
    }
   ],
   "source": [
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Markov Decision Process\n",
    "\n",
    "- State -> action -> state -> action ...\n",
    "- Agent\n",
    "- Set of actions\n",
    "- Transitions\n",
    "- Discount factor\n",
    "- Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability to discard words to reduce noise\n",
    "\n",
    "![prob](https://miro.medium.com/max/460/1*h4xJftToHQRc_sl1ejpmfA.png)\n",
    "\n",
    "Credits: https://towardsdatascience.com/how-to-train-custom-word-embeddings-using-gpu-on-aws-f62727a1e3f6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "\n",
    "A measure of how far a model's predictions are from its label.\n",
    "\n",
    "In contrast to:\n",
    "- reward function\n",
    "\n",
    "## SSE (sum of squared of the errors)\n",
    "\n",
    "## Mean Squared Errors (MSE)\n",
    "\n",
    "Mean Squared Error (MSE) is a common loss function used for regression problems.\n",
    "\n",
    "Mean squared error of an estimator measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value.\n",
    "\n",
    "Can be used for regression problems (say, to predict the price of a house).\n",
    "\n",
    "Alternatives:\n",
    "- Binary Crossentropy Loss (is better for dealing with probabilities)\n",
    "\n",
    "## Binary Crossentropy Loss\n",
    "\n",
    "Used in binary classification tasks, ie model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the binary_crossentropy loss function.\n",
    "\n",
    "## Cross-entropy loss\n",
    "\n",
    "## Sparse Categorical Crossentropy\n",
    "\n",
    "Used in image classification task\n",
    "\n",
    "## Log loss\n",
    "\n",
    "Used in logistic regression tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "This is how the model is updated based on the data it sees and its loss function.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Optimization algorithm for finding the minimum of a function.\n",
    "\n",
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "## Adam\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Frameworks\n",
    "- Keras (best learning tool for beginners)\n",
    "- PyTorch (dynamic)\n",
    "- Tensorflow (declerative programming, can run on Apache Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "- Binary\n",
    "- Not binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function\n",
    "\n",
    "A function (for example, ReLU or sigmoid) that takes in the weighted sum of all of the inputs from the previous layer and then generates and passes an output value (typically nonlinear) to the next layer.\n",
    "\n",
    "https://developers.google.com/machine-learning/glossary/#activation_function\n",
    "\n",
    "## Softmax Function\n",
    "\n",
    "A function that provides probabilities for each possible class in a multi-class classification model. The probabilities add up to exactly 1.0. For example, softmax might determine that the probability of a particular image being a dog at 0.9, a cat at 0.08, and a horse at 0.02.\n",
    "\n",
    "Example: last layer is a 10-node softmax layer—this returns an array of 10 probability scores that sum to 1.\n",
    "\n",
    "\n",
    "## Sigmoid\n",
    "\n",
    "A function that maps logistic or multinomial regression output (log odds) to probabilities, returning a value between 0 and 1\n",
    "\n",
    "Sigmoid function converts /sigma into a probability between 0 and 1.\n",
    "\n",
    "## ReLU (Rectified Linear Unit)\n",
    "\n",
    "- If input is negative or zero, output is 0.\n",
    "- If input is positive, output is equal to input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance measure\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Used when taining a neural network.\n",
    "\n",
    "- training loss decreases with each epoch\n",
    "- training accuracy increases with each epoch\n",
    "\n",
    "![acc](https://www.tensorflow.org/tutorials/keras/basic_text_classification_files/output_6hXx-xOv-llh_0.png)\n",
    "\n",
    "\n",
    "## Precision\n",
    "\n",
    "TP/(TP+FP)\n",
    "\n",
    "- TP=true positive\n",
    "- FP=false positive\n",
    "\n",
    "## Recall\n",
    "\n",
    "TP/(TP+FN)\n",
    "\n",
    "## F1 score\n",
    "\n",
    "(2 × Precision × Recall) / (Precision + Recall)\n",
    "\n",
    "## Mean Absolute Error\n",
    "\n",
    "A common regression metric is Mean Absolute Error (MAE).\n",
    "\n",
    "## Mean Squared Error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping\n",
    "\n",
    "Early stopping is a useful technique to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "## L1 Regularization\n",
    "penalizes weights in proportion to the sum of the absolute values of the weights\n",
    "\n",
    "https://developers.google.com/machine-learning/glossary/#L1_regularization\n",
    "\n",
    "## L2 Regularization\n",
    "\n",
    "penalizes weights in proportion to the sum of the squares of the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsity\n",
    "\n",
    "The number of elements set to zero (or null) in a vector or matrix divided by the total number of entries in that vector or matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking\n",
    "\n",
    "## Wilson-Score Interval\n",
    "\n",
    "Used by Reddit to rank comments.\n",
    "\n",
    "## Euclidean Ranking\n",
    "\n",
    "## Cosine Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNet + BERT in spacy\n",
    "\n",
    "https://spacy.io/models/en#en_pytt_xlnetbasecased_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table where each cell `[i,j]` indicates how often label `j` was predicted when the correct label was `i`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers\n",
    "\n",
    "- Every feature gets a say in determining which label should be assigned to a given input value. \n",
    "- To choose a label for an input value, the naive Bayes classifier begins by calculating the prior probability of each label, which is determined by checking frequency of each label in the training set.\n",
    "\n",
    "Credits: https://www.nltk.org/book/ch06.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cheat-sheet",
   "language": "python",
   "name": "nlp-cheat-sheet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
